# LLM configs

- `./presets.ini` follows the formats of [GGML's llama-server preset](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md#model-presets), declares all LLMs that can be seen by the `llama-server` backend
- `./config.json` configures the 
  - `config["LLM"]`: stores the number of instances to deploy for each LLM. Each storing `model_id : {"num_instance" : N}`
  - `config["API-port"]`: the port exposed by the router
  - `config["LLM-base-port"]`: starting port for `llama-server` instance. Multiple instances are spawned sequentially from `+0` to `+N-1`
  - `config["llama-server-executable"]`: location of llama-server binary, which should be generated by the base image `ghcr.io/ggml-org/llama.cpp:server-cuda`